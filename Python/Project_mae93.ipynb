{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the dataset and its corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 224, 224)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "filename = 'data.sav'\n",
    "loaded_data = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "filename = 'label.sav'\n",
    "loaded_label = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "print(np.shape(loaded_data))\n",
    "print(np.shape(loaded_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffling the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(0, 50000))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "data = loaded_data[indices]\n",
    "label = loaded_label[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting the dataset. One fifth of the dataset, i.e. 10000 images, are allocated as test data, while the other images are used for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = int((len(label)*4)/5)\n",
    "\n",
    "test_x = data[m:]\n",
    "test_y = label[m:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a convolutional neural network for 5 epochs with a mini-batch size of 64 using five-fold cross validation.\n",
    "\n",
    "The architecture is as follows:\n",
    "- One hidden convolutional layer with a kernel size of 3 and with 32 filters followed by a MaxPooling layer\n",
    "- One hidden dense neural netwok\n",
    "- One output layer\n",
    "- The activation of the hidden layers is a Relu\n",
    "- The activation of the output layer is a Softmax\n",
    "- The loss function is a categorical cross-entropy funtion\n",
    "- The optimizer of this model is Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mahdi\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten\n",
    "\n",
    "def createmodel():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(224, 224)))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 8000  8001  8002 ... 39997 39998 39999] TEST: [   0    1    2 ... 7997 7998 7999]\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/5\n",
      "32000/32000 [==============================] - 23s 717us/step - loss: 8.0850 - acc: 0.4980 - val_loss: 7.9482 - val_acc: 0.5069\n",
      "Epoch 2/5\n",
      "32000/32000 [==============================] - 22s 681us/step - loss: 8.0837 - acc: 0.4985 - val_loss: 7.9482 - val_acc: 0.5069\n",
      "Epoch 3/5\n",
      "32000/32000 [==============================] - 22s 680us/step - loss: 8.0837 - acc: 0.4985 - val_loss: 7.9482 - val_acc: 0.5069\n",
      "Epoch 4/5\n",
      "32000/32000 [==============================] - 22s 683us/step - loss: 8.0837 - acc: 0.4985 - val_loss: 7.9482 - val_acc: 0.5069\n",
      "Epoch 5/5\n",
      "32000/32000 [==============================] - 22s 680us/step - loss: 8.0837 - acc: 0.4985 - val_loss: 7.9482 - val_acc: 0.5069\n",
      "TRAIN: [    0     1     2 ... 39997 39998 39999] TEST: [ 8000  8001  8002 ... 15997 15998 15999]\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/5\n",
      "32000/32000 [==============================] - 22s 699us/step - loss: 8.0948 - acc: 0.4978 - val_loss: 7.9301 - val_acc: 0.5080\n",
      "Epoch 2/5\n",
      "32000/32000 [==============================] - 22s 687us/step - loss: 8.0948 - acc: 0.4978 - val_loss: 7.9301 - val_acc: 0.5080\n",
      "Epoch 3/5\n",
      "32000/32000 [==============================] - 22s 688us/step - loss: 8.0948 - acc: 0.4978 - val_loss: 7.9301 - val_acc: 0.5080\n",
      "Epoch 4/5\n",
      "32000/32000 [==============================] - 22s 689us/step - loss: 8.0948 - acc: 0.4978 - val_loss: 7.9301 - val_acc: 0.5080\n",
      "Epoch 5/5\n",
      "32000/32000 [==============================] - 22s 685us/step - loss: 8.0948 - acc: 0.4978 - val_loss: 7.9301 - val_acc: 0.5080\n",
      "TRAIN: [    0     1     2 ... 39997 39998 39999] TEST: [16000 16001 16002 ... 23997 23998 23999]\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/5\n",
      "32000/32000 [==============================] - 22s 702us/step - loss: 8.0671 - acc: 0.4995 - val_loss: 8.0389 - val_acc: 0.5012\n",
      "Epoch 2/5\n",
      "32000/32000 [==============================] - 22s 685us/step - loss: 8.0671 - acc: 0.4995 - val_loss: 8.0389 - val_acc: 0.5012\n",
      "Epoch 3/5\n",
      "32000/32000 [==============================] - 22s 690us/step - loss: 8.0671 - acc: 0.4995 - val_loss: 8.0389 - val_acc: 0.5012\n",
      "Epoch 4/5\n",
      "32000/32000 [==============================] - 22s 691us/step - loss: 8.0671 - acc: 0.4995 - val_loss: 8.0389 - val_acc: 0.5012\n",
      "Epoch 5/5\n",
      "32000/32000 [==============================] - 22s 686us/step - loss: 8.0671 - acc: 0.4995 - val_loss: 8.0389 - val_acc: 0.5012\n",
      "TRAIN: [    0     1     2 ... 39997 39998 39999] TEST: [24000 24001 24002 ... 31997 31998 31999]\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/5\n",
      "32000/32000 [==============================] - 24s 739us/step - loss: 8.0567 - acc: 0.5001 - val_loss: 8.0913 - val_acc: 0.4980\n",
      "Epoch 2/5\n",
      "32000/32000 [==============================] - 22s 697us/step - loss: 8.0540 - acc: 0.5003 - val_loss: 8.0913 - val_acc: 0.4980\n",
      "Epoch 3/5\n",
      "32000/32000 [==============================] - 22s 695us/step - loss: 8.0540 - acc: 0.5003 - val_loss: 8.0913 - val_acc: 0.4980\n",
      "Epoch 4/5\n",
      "32000/32000 [==============================] - 22s 687us/step - loss: 8.0540 - acc: 0.5003 - val_loss: 8.0913 - val_acc: 0.4980\n",
      "Epoch 5/5\n",
      "32000/32000 [==============================] - 22s 686us/step - loss: 8.0540 - acc: 0.5003 - val_loss: 8.0913 - val_acc: 0.4980\n",
      "TRAIN: [    0     1     2 ... 31997 31998 31999] TEST: [32000 32001 32002 ... 39997 39998 39999]\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/5\n",
      "32000/32000 [==============================] - 22s 697us/step - loss: 8.0606 - acc: 0.4999 - val_loss: 8.0409 - val_acc: 0.5011\n",
      "Epoch 2/5\n",
      "32000/32000 [==============================] - 22s 691us/step - loss: 8.0606 - acc: 0.4999 - val_loss: 8.0409 - val_acc: 0.5011\n",
      "Epoch 3/5\n",
      "32000/32000 [==============================] - 22s 689us/step - loss: 8.0606 - acc: 0.4999 - val_loss: 8.0409 - val_acc: 0.5011\n",
      "Epoch 4/5\n",
      "32000/32000 [==============================] - 22s 689us/step - loss: 8.0606 - acc: 0.4999 - val_loss: 8.0409 - val_acc: 0.5011\n",
      "Epoch 5/5\n",
      "32000/32000 [==============================] - 22s 689us/step - loss: 8.0606 - acc: 0.4999 - val_loss: 8.0409 - val_acc: 0.5011\n",
      "Best accuracy:  0.508\n",
      "Best loss:  7.930105356216431\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "X = data[:m]\n",
    "y = label[:m]\n",
    "kf = KFold(n_splits=5)\n",
    "best_accuracy = 0\n",
    "best_loss = 0\n",
    "best_model = createmodel()\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    mod = createmodel();\n",
    "    mod.fit(X_train, to_categorical(y_train),\n",
    "                      validation_data=(X_test, to_categorical(y_test)),\n",
    "                      epochs=5, batch_size=64, shuffle = True)\n",
    "    loss, accuracy = mod.evaluate(X_test, to_categorical(y_test), verbose=0)\n",
    "    if(accuracy > best_accuracy):\n",
    "        best_accuracy = accuracy\n",
    "        best_loss = loss\n",
    "        best_model = mod\n",
    "\n",
    "print(\"Best accuracy: \", best_accuracy)\n",
    "print(\"Best loss: \", best_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the best model gotten from cross validation on the test data and reporting the accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.5006\n",
      "Loss =  8.04937689666748\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = best_model.evaluate(test_x, to_categorical(test_y), verbose=0)\n",
    "print(\"Accuracy = \", accuracy)\n",
    "print(\"Loss = \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using randomized search to tune the hyperparameters of a random tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "def randomsearch(hyperparameter_grid):\n",
    "    clf = RandomForestClassifier(n_estimators=20)\n",
    "    rs = RandomizedSearchCV(estimator=clf, param_distributions=hyperparameter_grid, n_iter=20, cv=5)\n",
    "    return rs\n",
    "\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": sp_randint(1, 11),\n",
    "              \"min_samples_split\": sp_randint(2, 11),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "rs = randomsearch(param_dist).fit(X.reshape(40000, 50176), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the best estimator gotten from the randomized search and reporting the accuracy and f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Confusion Matrix | Actual Positive | Actual Negative\n",
      "Classified as Positive |            2861 |            1347\n",
      "Classified as Negative |            2145 |            3647\n",
      "Accuracy =  0.6508\n",
      "F score =  0.6486292455337377\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pred = rs.best_estimator_.predict(test_x.reshape(10000, 50176))\n",
    "tn, fp, fn, tp = confusion_matrix(test_y, pred).ravel()\n",
    "print(\"%22s | %15s | %15s\" % ('Confusion Matrix', 'Actual Positive', 'Actual Negative'))\n",
    "print(\"%22s | %15d | %15d\" % ('Classified as Positive', tp, fp))\n",
    "print(\"%22s | %15d | %15d\" % ('Classified as Negative', fn, tn))\n",
    "accuracy = sklearn.metrics.accuracy_score(test_y, pred)\n",
    "fscore = sklearn.metrics.f1_score(test_y, pred, average='macro')\n",
    "print(\"Accuracy = \", accuracy)\n",
    "print(\"F score = \", fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried using SVM models using rbf and polynomial kernels but neither of them converged. Methodology used is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 8000  8001  8002 ... 39997 39998 39999] TEST: [   0    1    2 ... 7997 7998 7999]\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = data[:m]\n",
    "y = label[:m]\n",
    "kf = KFold(n_splits=5)\n",
    "best_accuracy = 0\n",
    "best_f1score = 0\n",
    "best_suppvec = 0\n",
    "#best_model = SVC(C=0.001, kernel = 'rbf')\n",
    "best_model = SVC(C=0.001, kernel = 'poly')\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    #best_model = SVC(C=0.001, kernel = 'rbf')\n",
    "    svm = SVC(C=1e10, kernel = 'poly')\n",
    "    svm.fit(X_train.reshape(32000, 50176), y_train)\n",
    "    pred = model.predict(X_test.reshape(8000, 50176))\n",
    "    supportvectors_RSVC = svm.support_vectors_\n",
    "    accuracy_RSVC = sklearn.metrics.accuracy_score(valid_y, pred)\n",
    "    f1_score_RSVC = sklearn.metrics.f1_score(valid_y, pred, average='macro')\n",
    "    if(f1_score_RSVC > best_f1score):\n",
    "        best_accuracy = accuracy_RSVC\n",
    "        best_f1score = f1_score_RSVC\n",
    "        best_suppvec = len(supportvectors_RSVC)\n",
    "        best_model = svm\n",
    "\n",
    "print(\"Best Total Number of Support Vectors = \", best_suppvec)\n",
    "print(\"Best Accuracy = \", accuracy_RSVC)\n",
    "print(\"Best F1 Score = \", f1_score_RSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = best_model.predict(test_x.reshape(10000, 50176))\n",
    "print(\"Predicted class: \", pred)\n",
    "print()\n",
    "tn, fp, fn, tp = confusion_matrix(test_y, pred).ravel()\n",
    "print(\"%22s | %15s | %15s\" % ('Confusion Matrix', 'Actual Positive', 'Actual Negative'))\n",
    "print(\"%22s | %15d | %15d\" % ('Classified as Positive', tp, fp))\n",
    "print(\"%22s | %15d | %15d\" % ('Classified as Negative', fn, tn))\n",
    "print()\n",
    "accuracy = sklearn.metrics.accuracy_score(test_y, pred)\n",
    "f1_score = sklearn.metrics.f1_score(test_y, pred, average='macro')\n",
    "print(\"Accuracy = \", accuracy)\n",
    "print(\"F score = \", f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
